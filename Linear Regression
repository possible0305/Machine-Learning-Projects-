# Machine-Learning-Projects-


# coding: utf-8

# In[3]:

import numpy as np
import pandas as pd
from pandas import Series,DataFrame


# In[4]:

import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style('whitegrid')
get_ipython().magic(u'matplotlib inline')


# In[5]:

from sklearn.datasets import load_boston


# In[7]:

# Load the housing dataset
boston = load_boston()

print (boston.DESCR)


# In[8]:

# Histogram of prices (this is the target of our dataset)
plt.hist(boston.target,bins=60)

#label
plt.xlabel('Price in $1000s')
plt.ylabel('Number of houses')


# In[12]:

# Plot the column at the 5 index (Labeled RM)
plt.scatter(boston.data[:,5],boston.target)

#label
plt.ylabel('Price in $1000s')
plt.xlabel('Number of rooms')


# In[16]:

# reset data as pandas DataFrame
boston_df = DataFrame(boston.data)

# label columns
boston_df.columns = boston.feature_names

#show
boston_df.head()


# In[17]:

# Set price column for target
boston_df['Price'] = boston.target


# In[18]:

# Show result
boston_df.head()


# In[57]:

# Using seabron to create a linear fit
sns.lmplot('RM','Price',data = boston_df,scatter = True)


# In[30]:

# Set up X as median room values
X = boston_df.RM

#Use v to make X two-dimensional
X = np.vstack(boston_df.RM)

# Set up Y as the target price of the houses.
Y = boston_df.Price


# In[33]:

# Create the X array in the form [X 1]
X = np.array( [ [value,1] for value in X ] )


# In[32]:

print (X)


# In[34]:

# Now get out m and b values for our best fit line
m, b = np.linalg.lstsq(X, Y)[0]


# In[38]:

# First the original points, Price vs Avg Number of Rooms
plt.plot(boston_df.RM,boston_df.Price,'o')

# Next the best fit line
x= boston_df.RM
plt.plot(x, m*x + b,'r',label='Best Fit Line')


# In[46]:

# Get the resulting array
result = np.linalg.lstsq(X,Y)

# Get the total error
error_total = result[1]

# Get the root mean square error
rmse = np.sqrt(error_total/len(X) )

# Print
print ("The root mean squared error was %.2f " %rmse)


# In[55]:

print (result[3])


# In[58]:

# Import for Multivariate Linear Regression
import sklearn
from sklearn.linear_model import LinearRegression


# In[59]:

# Create a LinearRegression Object
lreg = LinearRegression()


# The functions we will be using are:
# 
# lreg.fit() which fits a linear model
# 
# lreg.predict() which is used to predict Y using the linear model with estimated coefficients
# 
# lreg.score() which returns the coefficient of determination (R^2). A measure of how well observed outcomes are replicated by the model, learn more about it here
# 
# We'll start the multi variable regression analysis by seperating our boston dataframe into the data columns and the target columns:

# In[64]:

# Data Columns
X_multi = boston_df.drop('Price',1)

# Targets
Y_target = boston_df.Price


# In[65]:

# Implement Linear Regression
lreg.fit(X_multi,Y_target)


# In[67]:

print (' The estimated intercept coefficient is %.2f ' %lreg.intercept_)


# In[69]:

print (' The number of coefficients used was %d ' % len(lreg.coef_))


# In[70]:

# Set a DataFrame from the Features
coeff_df = DataFrame(boston_df.columns)
coeff_df.columns = ['Features']

# Set a new column lining up the coefficients from the linear regression
coeff_df["Coefficient Estimate"] = pd.Series(lreg.coef_)

# Show
coeff_df


# In[71]:

# Grab the output and set as X and Y test and train data sets!
X_train, X_test, Y_train, Y_test = sklearn.cross_validation.train_test_split(X,boston_df.Price)


# In[73]:

# Print shapes of the training and testing data sets
print (X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)


# In[74]:

# Create our regression object
lreg = LinearRegression()

# Once again do a linear regression, except only on the training sets this time
lreg.fit(X_train,Y_train)


# In[80]:

# Predictions on training and testing sets
pred_train = lreg.predict(X_train)
pred_test = lreg.predict(X_test)



# In[83]:

print ("Fit a model X_train, and calculate MSE with Y_train: %.2f"  % np.mean((Y_train - pred_train) ** 2))
    
print ("Fit a model X_train, and calculate MSE with X_test and Y_test: %.2f"  %np.mean((Y_test - pred_test) ** 2))


# In[87]:

# Scatter plot the training data
train = plt.scatter(pred_train,(pred_train-Y_train),c='b',alpha=0.5)

# Scatter plot the testing data
test = plt.scatter(pred_test,(pred_test-Y_test),c='r',alpha=0.5)

# Plot a horizontal axis line at 0
plt.hlines(y=0,xmin=-10,xmax=50)

#Labels
plt.legend((train,test),('Training','Test'),loc='lower left')
plt.title('Residual Plots')


# In[ ]:



